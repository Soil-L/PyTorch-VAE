{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from models.vanilla_vae import VanillaVAE  # Adjust the import path as necessary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the model parameters (these should match the parameters used during training)\n",
    "in_channels = 3  # Example: 3 for RGB images\n",
    "latent_dim = 128  # Example: latent dimension size\n",
    "\n",
    "# Define the path to the checkpoint\n",
    "checkpoint_path = 'logs/VanillaVAE/version_22/checkpoints/last.ckpt'  # Replace with the actual path to your checkpoint file\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "state_dict = checkpoint['state_dict']\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('model.'):\n",
    "        new_state_dict[k[6:]] = v  # Remove 'model.' prefix\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "model = VanillaVAE(in_channels=in_channels, latent_dim=latent_dim)\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "img = model.sample(num_samples=1,current_device=device)  # Call the sample method to generate new images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the tensor to a numpy array and transpose the dimensions to (H, W, C)\n",
    "img_np = img.cpu().detach().numpy().squeeze()\n",
    "img_np = img_np.transpose(1, 2, 0)\n",
    "\n",
    "# Clip the values to be in the range [0, 1]\n",
    "img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9933, 0.9987, 0.9978,  ..., 0.9992, 0.9978, 0.9828],\n",
      "          [0.9981, 0.9998, 0.9998,  ..., 0.9999, 0.9997, 0.9956],\n",
      "          [0.9994, 0.9998, 0.9999,  ..., 0.9999, 0.9998, 0.9958],\n",
      "          ...,\n",
      "          [0.9983, 0.9998, 0.9998,  ..., 1.0000, 0.9999, 0.9977],\n",
      "          [0.9991, 0.9998, 0.9999,  ..., 1.0000, 0.9999, 0.9983],\n",
      "          [0.9822, 0.9971, 0.9968,  ..., 0.9988, 0.9977, 0.9870]],\n",
      "\n",
      "         [[0.9884, 0.9994, 0.9980,  ..., 0.9996, 0.9982, 0.9712],\n",
      "          [0.9987, 0.9999, 0.9998,  ..., 1.0000, 0.9998, 0.9936],\n",
      "          [0.9925, 0.9997, 0.9997,  ..., 0.9998, 0.9996, 0.9909],\n",
      "          ...,\n",
      "          [0.9991, 0.9999, 0.9999,  ..., 1.0000, 1.0000, 0.9966],\n",
      "          [0.9942, 0.9998, 0.9997,  ..., 1.0000, 0.9999, 0.9969],\n",
      "          [0.9830, 0.9979, 0.9984,  ..., 0.9992, 0.9987, 0.9679]],\n",
      "\n",
      "         [[0.9911, 0.9989, 0.9982,  ..., 0.9992, 0.9981, 0.9649],\n",
      "          [0.9984, 1.0000, 0.9997,  ..., 1.0000, 0.9997, 0.9971],\n",
      "          [0.9982, 0.9998, 1.0000,  ..., 0.9998, 0.9999, 0.9898],\n",
      "          ...,\n",
      "          [0.9985, 1.0000, 0.9998,  ..., 1.0000, 0.9999, 0.9985],\n",
      "          [0.9977, 0.9998, 0.9999,  ..., 1.0000, 1.0000, 0.9950],\n",
      "          [0.9712, 0.9954, 0.9978,  ..., 0.9983, 0.9987, 0.9851]]]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "samples = model.sample(1, current_device=device)\n",
    "print(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
